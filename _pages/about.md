---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
Xiaonan Nie (聂小楠)
====
Email: xiaonan.nie [AT] pku.edu.cn

I am currently a fourth-year PhD student (2019 -- 2024) advised by Prof. [Bin Cui](https://cuibinpku.github.io) at School of Computer Science, Peking University. My research interests include Large-Scale Deep Learning Systems, Distributed Computing, Heterogeneous Computing and Sparse Neural Network. I am the main developer of [Hetu](https://hsword.github.io/projects/hetu/), an highly efficient distributed deep learning system.

Now, I am interned at the Machine Learning Platform Department (MLPD) of TEG at Tencent. I lead the development of a Large-Scale Pre-training System, Angel-PTM, which supports the training of trillion-level models(e.g., HunYuan-NLP 1T, Top-1 model in [CLUE](https://cluebenchmarks.com/rank.html)). I was a research intern in [System Research Group](https://www.microsoft.com/en-us/research/group/systems-and-networking-research-group-asia/) of Microsoft Research Asia (MSRA), working with [Lingxiao Ma](https://xysmlx.github.io), [Jilong Xue](https://www.microsoft.com/en-us/research/people/jxue/), [Shijie Cao](https://www.microsoft.com/en-us/research/people/shijiecao/) and [Youshan Miao](https://www.microsoft.com/en-us/research/people/yomia/), where we focused on training sparse MoE models efficiently.

News
=====
+ Oct 11th,2022: I was invited to present [EvoMoE](https://arxiv.org/abs/2112.14397) at [Google Workshop on Sparsity and Adaptive Computation](https://rsvp.withgoogle.com/events/googleworkshopsparsityadaptivecomputation-2022).

+ Aug 24th, 2022: We won the Best Scalable Data Science Paper Award of [VLDB 2022](https://vldb.org/2022/?conference-awards)!

+ Jan 20th, 2022: We won Outstanding Award & Champion of [2021 CCF BDCI Contest](https://mp.weixin.qq.com/s/hSoDMVMZApQxaiNqh2jUSg). (1st/33757).

Publications
=====

2023
======
1. **FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement.** [To appear]<br>
  **Xiaonan Nie**,  Xupeng Miao, Zilong Wang,  Jilong Xue, Lingxiao Ma, Zichao Yang, Gang Cao and Bin Cui.<br>
  ACM SIGMOD International Conference on Management of Data. <br>
  ***SIGMOD 2023, CCF-A.***

2. **Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism.** [[PDF]](https://arxiv.org/abs/2211.13878)<br>
  Xupeng Miao, Yujie Wang, Youhe Jiang,  Chunan Shi, **Xiaonan Nie**, Hailin Zhang and Bin Cui.<br>
  International Conference on Very Large Data Bases. <br>
  ***VLDB 2023, CCF-A.***

2022
======
1. **TSPLIT: Fine-grained GPU Memory Management for Efficient DNN Training via Tensor Splitting.** [[PDF]](https://ieeexplore.ieee.org/document/9835178)<br>
  **Xiaonan Nie**,  Xupeng Miao, Zhi Yang and Bin Cui.<br>
  IEEE International Conference on Data Engineering. <br>
  ***ICDE 2022, CCF-A.*** <br>

1. **EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate.** [[PDF]](https://arxiv.org/abs/2112.14397)<br>
  **Xiaonan Nie**, Xupeng Miao, Shijie Cao, Lingxiao Ma, Qibin Liu, Jilong Xue, Youshan Miao, Yi Liu, Zhi Yang and Bin Cui.<br>
  Google Workshop on Sparsity and Adaptive Computation. <br>

1. **Hetu: A highly efficient automatic parallel distributed deep learning system.** [[PDF]](http://scis.scichina.com/en/2023/117101.pdf)<br>
  Xupeng Miao, ***Xiaonan Nie***, Hailin Zhang, Tong Zhao and Bin Cui.<br>
  Science China Information Sciences.<br>
  **SCIS 2022, CCF-A.** <br>

1. **HET: Scaling out Huge Embedding Model Training via Cache-enabled Distributed Framework.** [[PDF]](https://dl.acm.org/doi/10.14778/3489496.3489511)<br>
  Xupeng Miao, Hailin Zhang, Yining Shi,  **Xiaonan Nie**, Zhi Yang, Yangyu Tao and Bin Cui. <br>
  International Conference on Very Large Data Bases. <br>
  ***VLDB 2022, CCF-A,*** **<font color=purple>Best Scalable Data Science Paper!</font>** <br>
1. **HET-GMP: A Graph-based System Approach to Scaling Large Embedding Model Training.** [[PDF]](https://dl.acm.org/doi/10.1145/3514221.3517902)
  Xupeng Miao, Yining Shi, Hailin Zhang,  Xin Zhang, **Xiaonan Nie**, Zhi Yang and Bin Cui.<br>
  ACM SIGMOD International Conference on Management of Data. <br>
  ***SIGMOD 2022, CCF-A.*** <br>
1. **OSDP: Optimal Sharded Data Parallel for Distributed Deep Learning.** [[PDF]](https://arxiv.org/abs/2209.13258)<br>
  Youhe Jiang,  Xupeng Miao, **Xiaonan Nie** and Bin Cui.<br>
  International Conference on Machine Learning. <br>
  ***ICML 2022, CCF-A.*** <br>


2021
=====
1. **Heterogeneity-Aware Distributed Machine Learning Training via Partial Reduce.** [[PDF]](https://dl.acm.org/doi/10.1145/3448016.3452773)<br>
  Xupeng Miao, **Xiaonan Nie**, Yingxia Shao, Zhi Yang, Jiawei Jiang, Lingxiao Ma and Bin Cui.<br>
  ACM SIGMOD International Conference on Management of Data(HEAT Workshop).<br> 
  ***SIGMOD 2021, CCF-A.*** <br>

Awards
====
+ [CCF BDCI](https://www.datafountain.cn/special/BDCI2021) Outstanding Award & Champion, 2021.
+ [HUAWEI DIGIX Global AI Challenge](https://developer.huawei.com/consumer/cn/activity/digixActivity/digixWinnersDetail/201621215957378831), First runner-up of *Search rankings in multimodal and multilingual contexts*, 2021.
+ ACM-ICPC Asia Regional Contest, Silver Medal, 2017.
+ National Scholarship, 2016.