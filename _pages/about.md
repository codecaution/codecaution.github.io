---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
Xiaonan Nie (聂小楠)
====
Email: xiaonan.nie [AT] pku.edu.cn

Xiaonan Nie is currently a final-year PhD student under the supervision of Professor [Bin Cui](https://cuibinpku.github.io) at [DAIR Lab](https://github.com/PKU-DAIR), Computer Science Department, Peking University.  

His research interests primarily lie in the field of Distributed Deep Learning Systems, focusing on Large Language Model (LLM) systems, Multi-Agent Systems, Heterogeneous Computing, and Sparse Neural Networks, specifically Mixture of Experts (MoE). During his PhD, his research has concentrated on Hetu, a highly efficient distributed deep learning system, where he also served as the main developer.


As a research intern at Tencent's Machine Learning Platform Department (2022-2023), he led the development of Angel-PTM, a large-scale pre-training system designed for trillion-scale models. Additionally, he was a research intern in [System Research Group](https://www.microsoft.com/en-us/research/group/systems-and-networking-research-group-asia/) of Microsoft Research Asia (MSRA) during 2021-2022, under the supervision of Dr. Jilong Xue and Dr. Lingxiao Ma.

<!-- Now, I am interned at the Machine Learning Platform Department (MLPD) of TEG at Tencent, where I lead the development of a Large-Scale Pre-training System. 
Previous, I was a research intern in [System Research Group](https://www.microsoft.com/en-us/research/group/systems-and-networking-research-group-asia/) of Microsoft Research Asia (MSRA), working with [Lingxiao Ma](https://xysmlx.github.io), [Jilong Xue](https://www.microsoft.com/en-us/research/people/jxue/), [Shijie Cao](https://www.microsoft.com/en-us/research/people/shijiecao/) and [Youshan Miao](https://www.microsoft.com/en-us/research/people/yomia/), where we focused on training sparse MoE models efficiently. -->

News
=====

+ Mar 19th, 2024: Fortunately, I have been invited to give a talk about LLM inference on NVIDIA [GTC 2024](https://www.nvidia.com/gtc/).
  
+ May 13th, 2023 : One paper was accepted by ***VLDB 2023***.

+ Apr 20th, 2023 : One paper was accepted by ***IJCAI 2023***.

+ Dec 13th, 2022: One paper was accepted by ***SIGMOD 2023***.

+ Oct 16th, 2022: One paper was accepted by ***VLDB 2023***.
  
+ Oct 11th, 2022: I was invited to present [EvoMoE](https://arxiv.org/abs/2112.14397) at ***Google Workshop on Sparsity and Adaptive Computation***.
<!-- + (https://rsvp.withgoogle.com/events/googleworkshopsparsityadaptivecomputation-2022). -->

+ Aug 24th, 2022: We won the Best Scalable Data Science Paper Award of ***VLDB 2022***.
<!-- + (https://vldb.org/2022/?conference-awards)! -->

+ Jan 20th, 2022: We won Outstanding Award & Champion of [2021 CCF BDCI Contest](https://mp.weixin.qq.com/s/hSoDMVMZApQxaiNqh2jUSg). (1st/33757).

Systems
=====
+ **Hetu: A Highly Efficient Automatic Parallel Distributed Deep Learning System**
  + [2021 Synced Machine Intelligence TOP-10 Open Source Awards.](https://www.jiqizhixin.com/awards/2021/events)
  + [Pop SOTA！List for AI Developers 2021.](https://mp.weixin.qq.com/s/jHkF9UpgEn1MLZpRH2FOaA)
  + Outstanding Award & Champion of [[2021 CCF BDCI Contest]](https://mp.weixin.qq.com/s/hSoDMVMZApQxaiNqh2jUSg)


+ **Angel-PTM: A Scalable and Economical Large-scale Pre-training System in Tencent**
  + Supports the training of trillion-level models (e.g., [HunYuan-NLP 1T, Top-1 model in CLUE](https://cluebenchmarks.com/rank.html))
  


Publications
=====

2023
======
1. **Angel-PTM: A Scalable and Economical Large-scale Pre-training System in Tencent.** [[PDF]](https://arxiv.org/pdf/2303.02868.pdf)<br>
   **Xiaonan Nie**,  Yi Liu, Fangcheng Fu, Jinbao Xue, Dian Jiao, Xupeng Miao, Yangyu Tao, and Bin Cui.<br>
   International Conference on Very Large Data Bases. <br>
   ***VLDB 2023, CCF-A.*** 

2. **FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement.** [[PDF]](https://arxiv.org/abs/2304.03946)<br>
  **Xiaonan Nie**,  Xupeng Miao, Zilong Wang,  Jilong Xue, Lingxiao Ma, Zichao Yang, Gang Cao and Bin Cui.<br>
  ACM SIGMOD International Conference on Management of Data. <br>
  ***SIGMOD 2023, CCF-A.***

1. **Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism.** [[PDF]](https://arxiv.org/abs/2211.13878)<br>
  Xupeng Miao, Yujie Wang, Youhe Jiang,  Chunan Shi, **Xiaonan Nie**, Hailin Zhang and Bin Cui.<br>
  International Conference on Very Large Data Bases. <br>
  ***VLDB 2023, CCF-A.*** 

1. **OSDP: Optimal Sharded Data Parallel for Distributed Deep Learning.** [[PDF]](https://arxiv.org/abs/2209.13258)<br>
  Youhe Jiang,  Fangcheng Fu, Xupeng Miao, **Xiaonan Nie** and Bin Cui.<br>
  International Joint Conference on Artificial Intelligence. <br>
  ***IJCAI 2023, CCF-A.***

2022
======
1. **TSPLIT: Fine-grained GPU Memory Management for Efficient DNN Training via Tensor Splitting.** [[PDF]](https://ieeexplore.ieee.org/document/9835178)<br>
  **Xiaonan Nie**,  Xupeng Miao, Zhi Yang and Bin Cui.<br>
  IEEE International Conference on Data Engineering. <br>
  ***ICDE 2022, CCF-A.*** <br>

2. **EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate.** [[PDF]](https://arxiv.org/abs/2112.14397)<br>
  **Xiaonan Nie**, Xupeng Miao, Shijie Cao, Lingxiao Ma, Qibin Liu, Jilong Xue, Youshan Miao, Yi Liu, Zhi Yang and Bin Cui.<br>
  Google Workshop on Sparsity and Adaptive Computation. <br>

3. **Hetu: A highly efficient automatic parallel distributed deep learning system.** [[PDF]](http://scis.scichina.com/en/2023/117101.pdf)<br>
  Xupeng Miao, ***Xiaonan Nie***, Hailin Zhang, Tong Zhao and Bin Cui.<br>
  Science China Information Sciences.<br>
  **SCIS 2022, CCF-A.** <br>

4. **HET: Scaling out Huge Embedding Model Training via Cache-enabled Distributed Framework.** [[PDF]](https://dl.acm.org/doi/10.14778/3489496.3489511) <br>
  Xupeng Miao, Hailin Zhang, Yining Shi,  **Xiaonan Nie**, Zhi Yang, Yangyu Tao and Bin Cui. <br>
  International Conference on Very Large Data Bases. <br>
  ***VLDB 2022, CCF-A,*** **<font color=purple>Best Scalable Data Science Paper!</font>**

5. **HET-GMP: A Graph-based System Approach to Scaling Large Embedding Model Training.** [[PDF]](https://dl.acm.org/doi/10.1145/3514221.3517902)<br>
  Xupeng Miao, Yining Shi, Hailin Zhang,  Xin Zhang, **Xiaonan Nie**, Zhi Yang and Bin Cui.<br>
  ACM SIGMOD International Conference on Management of Data. <br>
  ***SIGMOD 2022, CCF-A.***



2021
=====
1. **Heterogeneity-Aware Distributed Machine Learning Training via Partial Reduce.** [[PDF]](https://dl.acm.org/doi/10.1145/3448016.3452773)<br>
  Xupeng Miao, **Xiaonan Nie**, Yingxia Shao, Zhi Yang, Jiawei Jiang, Lingxiao Ma and Bin Cui.<br>
  ACM SIGMOD International Conference on Management of Data.<br> 
  ***SIGMOD 2021, CCF-A.*** <br>

Awards
====
+ The first level of [Huawei Top Minds](https://career.huawei.com/reccampportal/portal5/topminds.html) (华为天才少年), 2023.
+ [CCF BDCI](https://www.datafountain.cn/special/BDCI2021) Outstanding Award & Champion, 2021.
+ [HUAWEI DIGIX Global AI Challenge](https://developer.huawei.com/consumer/cn/activity/digixActivity/digixWinnersDetail/201621215957378831), First runner-up of *Search rankings in multimodal and multilingual contexts*, 2021.
+ ACM-ICPC Asia Regional Contest, Silver Medal, 2017.
+ National Scholarship, 2016.