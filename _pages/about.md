---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
Xiaonan Nie (聂小楠)
====
Email: xiaonan.nie [AT] pku.edu.cn

I am currently a fourth-year PhD student (2019 -- 2024) advised by Prof. [Bin Cui](https://cuibinpku.github.io) at School of Computer Science, Peking University. My research interests include Large-Scale Deep Learning Systems, Distributed Computing, Heterogeneous Computing and Sparse Neural Network. I am the main developer of [Hetu](https://hsword.github.io/projects/hetu/), an highly efficient distributed deep learning system.

Now, I am interned at the Machine Learning Platform Department (MLPD) of TEG at Tencent. I lead the development of a Large-Scale Pre-training System, Angel-PTM, which supports the training of trillion-level models(e.g., HunYuan_nlp in [CLUE](https://cluebenchmarks.com/rank.html)). I was a research intern in [System Research Group](https://www.microsoft.com/en-us/research/group/systems-and-networking-research-group-asia/) of Microsoft Research Asia (MSRA), working with [Lingxiao Ma](https://xysmlx.github.io), [Jilong Xue](https://www.microsoft.com/en-us/research/people/jxue/), [Shijie Cao](https://www.microsoft.com/en-us/research/people/shijiecao/) and [Youshan Miao](https://www.microsoft.com/en-us/research/people/yomia/), where we focused on training sparse MoE models efficiently.

News
=====
+ Oct 11th,2022: I was invited to present [EvoMoE](https://arxiv.org/abs/2112.14397) at [Google Workshop on Sparsity and Adaptive Computation](https://rsvp.withgoogle.com/events/googleworkshopsparsityadaptivecomputation-2022).

+ Aug 24th, 2022: We won the Best Scalable Data Science Paper Award of [VLDB 2022](https://vldb.org/2022/?conference-awards)!

+ Jan 20th, 2022: We won Outstanding Award & Champion of [2021 CCF BDCI Contest](https://mp.weixin.qq.com/s/hSoDMVMZApQxaiNqh2jUSg). (1st/33757).

Publications
=====

2023
======
+ **FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement.**

  <u>Xiaonan Nie</u>,  Xupeng Miao, Zilong Wang,  Jilong Xue, Lingxiao Ma, Zichao Yang, Gang Cao and Bin Cui

  *In Proceedings of SIGMOD Conference 2023*

+ **Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism.** 

  Xupeng Miao, Yujie Wang, Youhe Jiang,  Chunan Shi, <u>Xiaonan Nie</u>, Hailin Zhang and Bin Cui

  *Proc. VLDB Endow. 2023* 

2022
======
+ **TSPLIT: Fine-grained GPU Memory Management for Efficient DNN Training via Tensor Splitting.** [[PDF]](https://ieeexplore.ieee.org/document/9835178)

  <u>Xiaonan Nie</u>,  Xupeng Miao, Zhi Yang and Bin Cui

  *In Proceedings of ICDE Conference 2022*

+ **EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate.** [[PDF]](https://arxiv.org/abs/2112.14397)

  <u>Xiaonan Nie</u>, Xupeng Miao, Shijie Cao, Lingxiao Ma, Qibin Liu, Jilong Xue, Youshan Miao, Yi Liu, Zhi Yang and Bin Cui

  *Google Workshop on Sparsity and Adaptive Computation 2022* 

+ **Hetu: A highly efficient automatic parallel distributed deep learning system.**

  Xupeng Miao, <u>Xiaonan Nie</u>, Hailin Zhang, Tong Zhao and Bin Cui

  *Sci. China Inf. Sci. 2022*

+ **HET: Scaling out Huge Embedding Model Training via Cache-enabled Distributed Framework.** [[PDF]](https://dl.acm.org/doi/10.14778/3489496.3489511)

  Xupeng Miao, Hailin Zhang, Yining Shi,  <u>Xiaonan Nie</u>, Zhi Yang, Yangyu Tao and Bin Cui

  *Proc. VLDB Endow. 2022* (<font color=purple>Best Scalable Data Science Paper</font>)

+ **HET-GMP: A Graph-based System Approach to Scaling Large Embedding Model Training.** [[PDF]](https://dl.acm.org/doi/10.1145/3514221.3517902)

  Xupeng Miao, Yining Shi, Hailin Zhang,  Xin Zhang, <u>Xiaonan Nie</u>, Zhi Yang and Bin Cui

  *In Proceedings of SIGMOD Conference 2022*

+ **OSDP: Optimal Sharded Data Parallel for Distributed Deep Learning.** [[PDF]](https://arxiv.org/abs/2209.13258)

  Youhe Jiang,  Xupeng Miao, <u>Xiaonan Nie</u> and Bin Cui

  *In Proceedings of ICML Hardware Aware Efficient Training (HAET) Workshop 2022*


2021
=====
+ **Heterogeneity-Aware Distributed Machine Learning Training via Partial Reduce.** [[PDF]](https://dl.acm.org/doi/10.1145/3448016.3452773)

  Xupeng Miao, <u>Xiaonan Nie</u>, Yingxia Shao, Zhi Yang, Jiawei Jiang, Lingxiao Ma and Bin Cui

  *In Proceedings of SIGMOD Conference 2021*

Awards
====
+ [CCF BDCI](https://www.datafountain.cn/special/BDCI2021) Outstanding Award & Champion, 2021.
+ [HUAWEI DIGIX Global AI Challenge](https://developer.huawei.com/consumer/cn/activity/digixActivity/digixWinnersDetail/201621215957378831), First runner-up of *Search rankings in multimodal and multilingual contexts*, 2021.
+ ACM-ICPC Asia Regional Contest, Silver Medal, 2017.
+ National Scholarship, 2016.